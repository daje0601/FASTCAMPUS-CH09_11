{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342eb8b-b605-4bd4-97fb-cd38974a364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"victorcallejasf/multimodal-hate-speech\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017eda4d-b972-4059-aa8d-6229a5a47b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/kagglehub/datasets/victorcallejasf/multimodal-hate-speech/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd /root/.cache/kagglehub/datasets/victorcallejasf/multimodal-hate-speech/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c54f441-5874-4ce4-a736-52be801f90d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 61220\n",
      "-rw-r--r-- 1 root 51426208 Feb 23 14:31 MMHS150K_GT.json\n",
      "-rw-r--r-- 1 root     1079 Feb 23 14:31 MMHS150K_readme.txt\n",
      "-rw-r--r-- 1 root      952 Feb 23 14:31 hatespeech_keywords.txt\n",
      "drwxr-xr-x 2 root  6144000 Feb 23 14:32 \u001b[0m\u001b[01;34mimg_resized\u001b[0m/\n",
      "drwxr-xr-x 2 root  2428928 Feb 23 14:32 \u001b[01;34mimg_txt\u001b[0m/\n",
      "drwxr-xr-x 2 root       82 Feb 23 14:32 \u001b[01;34msplits\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35d3a7-8023-40a5-81e6-5bf81694753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/CookBook/chapter09-Multi_Modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea933e5-9d51-45e4-ac1c-a98aa642f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"/workspace/CookBook/chapter09-Multi_Modal/final_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417722ce-5f43-4c1f-8d28-3df974c7688e",
   "metadata": {},
   "source": [
    "약 6G정도의 데이터를 다운 받습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de903ec-9b2a-4bbf-9604-0965f2781448",
   "metadata": {},
   "source": [
    "약 1분정도 소요됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d5102c-61ce-4e1c-9b5a-303fe667de7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>file_id</th>\n",
       "      <th>img_url</th>\n",
       "      <th>labels</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>labels_str</th>\n",
       "      <th>file_path</th>\n",
       "      <th>is_hate</th>\n",
       "      <th>original</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11872</td>\n",
       "      <td>1106904873637953537</td>\n",
       "      <td>http://pbs.twimg.com/tweet_video_thumb/D1yEG7V...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11069048736379...</td>\n",
       "      <td>Also woke up to this nigga asking me out on a ...</td>\n",
       "      <td>['NotHate', 'NotHate', 'NotHate']</td>\n",
       "      <td>./img_resized/1106904873637953537.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Also woke up to this nigga asking me out on a ...</td>\n",
       "      <td>또 이 사람한테 데이트 신청 받는 걸로 일어났어... 하지만 아니야. 난 더 이상 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381</td>\n",
       "      <td>1108495255819730945</td>\n",
       "      <td>http://pbs.twimg.com/media/D2Iqj05X0AAzksK.jpg</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://twitter.com/user/status/11084952558197...</td>\n",
       "      <td>#RNS REAL NIGGA SHIT</td>\n",
       "      <td>['NotHate', 'NotHate', 'NotHate']</td>\n",
       "      <td>./img_resized/1108495255819730945.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>#RNS REAL NIGGA SHIT</td>\n",
       "      <td>#RNS 진짜 새끼들 일</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              file_id  \\\n",
       "0  11872  1106904873637953537   \n",
       "1    381  1108495255819730945   \n",
       "\n",
       "                                             img_url     labels  \\\n",
       "0  http://pbs.twimg.com/tweet_video_thumb/D1yEG7V...  [0, 0, 0]   \n",
       "1     http://pbs.twimg.com/media/D2Iqj05X0AAzksK.jpg  [0, 0, 0]   \n",
       "\n",
       "                                           tweet_url  \\\n",
       "0  https://twitter.com/user/status/11069048736379...   \n",
       "1  https://twitter.com/user/status/11084952558197...   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Also woke up to this nigga asking me out on a ...   \n",
       "1                               #RNS REAL NIGGA SHIT   \n",
       "\n",
       "                          labels_str                              file_path  \\\n",
       "0  ['NotHate', 'NotHate', 'NotHate']  ./img_resized/1106904873637953537.jpg   \n",
       "1  ['NotHate', 'NotHate', 'NotHate']  ./img_resized/1108495255819730945.jpg   \n",
       "\n",
       "   is_hate                                           original  \\\n",
       "0    False  Also woke up to this nigga asking me out on a ...   \n",
       "1    False                               #RNS REAL NIGGA SHIT   \n",
       "\n",
       "                                          translated  \n",
       "0  또 이 사람한테 데이트 신청 받는 걸로 일어났어... 하지만 아니야. 난 더 이상 ...  \n",
       "1                                      #RNS 진짜 새끼들 일  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e66d46b-104b-4994-a235-57abcd75ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_hate\"] = df[\"is_hate\"].astype(int)\n",
    "df[\"file_path\"] = df[\"file_path\"].map(lambda x : x.replace(\"./\", \"/root/.cache/kagglehub/datasets/victorcallejasf/multimodal-hate-speech/versions/1/\"))\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1443b67d-618a-412b-8ae6-0fac9c1e45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4861e1-af7f-415a-bd99-46663a8ef713",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"입력으로 이미지 또는 ###텍스트가 주어지면, 해당 콘텐츠가 유해한지 여부를 \"1\" 또는 \"0\"으로 출력하세요.\n",
    "- 출력은 반드시 숫자 하나만(문자열 없이) 출력합니다.\n",
    "  - \"1\" : 유해(혐오·차별·폭력·선정성·기타 해악 가능성이 있다고 판단되는 경우)\n",
    "  - \"0\" : 비유해(문제 없다고 판단되는 경우)\n",
    "유해성(harmfulness)을 판단할 때는 아래 항목을 포함해 검토하세요:\n",
    "1. **혐오 표현 (Hate Speech)**  \n",
    "   - 인종, 성별, 종교, 성적 지향, 장애 등을 근거로 특정 집단/개인을 비하·모욕·차별하는 언어/이미지\n",
    "   - 명백한 증오, 차별, 폭력 조장 등\n",
    "2. **직접적인 폭력·잔혹 행위**  \n",
    "   - 극단적인 폭력 묘사(살해·자해·테러 등)\n",
    "   - 노골적으로 잔혹하거나 충격적인 내용\n",
    "3. **노골적 성적 콘텐츠**  \n",
    "   - 명백히 선정적·음란한 묘사, 아동 성착취 등 불법적이거나 심각한 윤리적 문제가 있는 경우\n",
    "4. **사생활 침해, 불법 행위 조장**  \n",
    "   - 명백한 사생활 침해, 범죄 또는 불법 행위를 조장하는 내용\n",
    "5. **기타 위험 요소**  \n",
    "   - 위 항목 외에도, 사회적으로 유해하거나 심각한 해악을 유발할 수 있는 이미지·텍스트\n",
    "만약 콘텐츠가 위 기준 중 하나라도 충족하여 \"유해\"하다고 판단되면 \"1\"을,  \n",
    "해당되지 않으면 \"0\"을 출력하십시오.\n",
    "출력은 오직 숫자 하나(1 또는 0)만 반환하고, 어떠한 추가 문구나 설명도 첨부하지 마십시오.\n",
    "\n",
    "###텍스트 :{text}\"\"\"\n",
    "\n",
    "system_message = \"당신은 이미지와 텍스트의 '유해성(harmfulness)'을 판별하는 분류 모델입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45727bb7-3270-449f-82ae-9de1b23b2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋을 OpenAI 메시지 형식으로 변환하는 함수      \n",
    "def format_data(sample):\n",
    "   return {\"messages\": [\n",
    "               {\n",
    "                   \"role\": \"system\", # 시스템 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": system_message}], # 시스템 메시지\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"user\",  # 사용자 역할\n",
    "                   \"content\": [\n",
    "                       {\n",
    "                           \"type\": \"text\",\n",
    "                           # 제품명과 카테고리를 포함한 프롬프트 생성\n",
    "                           \"text\": prompt.format(text=sample[\"translated\"]),\n",
    "                       },{\n",
    "                           \"type\": \"image\", # 이미지 타입\n",
    "                           \"image\": sample[\"file_path\"], # 제품 이미지\n",
    "                       }\n",
    "                   ],\n",
    "               },\n",
    "               {\n",
    "                   \"role\": \"assistant\", # AI 어시스턴트 역할\n",
    "                   \"content\": [{\"type\": \"text\", \"text\": sample[\"is_hate\"]}], # 제품 설명\n",
    "               },\n",
    "           ],\n",
    "       }\n",
    "\n",
    "\n",
    "dataset = [format_data(sample) for sample in hf_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f5ec87-3a57-4514-b170-d6cfe5098218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"당신은 이미지와 텍스트의 '유해성(harmfulness)'을 판별하는 분류 모델입니다.\"}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '입력으로 이미지 또는 ###텍스트가 주어지면, 해당 콘텐츠가 유해한지 여부를 \"1\" 또는 \"0\"으로 출력하세요.\\n- 출력은 반드시 숫자 하나만(문자열 없이) 출력합니다.\\n  - \"1\" : 유해(혐오·차별·폭력·선정성·기타 해악 가능성이 있다고 판단되는 경우)\\n  - \"0\" : 비유해(문제 없다고 판단되는 경우)\\n유해성(harmfulness)을 판단할 때는 아래 항목을 포함해 검토하세요:\\n1. **혐오 표현 (Hate Speech)**  \\n   - 인종, 성별, 종교, 성적 지향, 장애 등을 근거로 특정 집단/개인을 비하·모욕·차별하는 언어/이미지\\n   - 명백한 증오, 차별, 폭력 조장 등\\n2. **직접적인 폭력·잔혹 행위**  \\n   - 극단적인 폭력 묘사(살해·자해·테러 등)\\n   - 노골적으로 잔혹하거나 충격적인 내용\\n3. **노골적 성적 콘텐츠**  \\n   - 명백히 선정적·음란한 묘사, 아동 성착취 등 불법적이거나 심각한 윤리적 문제가 있는 경우\\n4. **사생활 침해, 불법 행위 조장**  \\n   - 명백한 사생활 침해, 범죄 또는 불법 행위를 조장하는 내용\\n5. **기타 위험 요소**  \\n   - 위 항목 외에도, 사회적으로 유해하거나 심각한 해악을 유발할 수 있는 이미지·텍스트\\n만약 콘텐츠가 위 기준 중 하나라도 충족하여 \"유해\"하다고 판단되면 \"1\"을,  \\n해당되지 않으면 \"0\"을 출력하십시오.\\n출력은 오직 숫자 하나(1 또는 0)만 반환하고, 어떠한 추가 문구나 설명도 첨부하지 마십시오.\\n\\n###텍스트 :시발 그녀'},\n",
       "    {'type': 'image',\n",
       "     'image': '/root/.cache/kagglehub/datasets/victorcallejasf/multimodal-hate-speech/versions/1/img_resized/1114286020797837312.jpg'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 1}]}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "533108f2-a79b-4d0c-a45f-4bb58f240550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test_size=0.1로 설정하여 전체 데이터의 10%를 테스트 세트로 분리\n",
    "train_dataset, test_dataset = train_test_split(dataset, \n",
    "                                             test_size=0.1, \n",
    "                                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89bf0a57-e6ae-4f96-8fdd-cf558db79a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': \"당신은 이미지와 텍스트의 '유해성(harmfulness)'을 판별하는 분류 모델입니다.\"}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '입력으로 이미지 또는 ###텍스트가 주어지면, 해당 콘텐츠가 유해한지 여부를 \"1\" 또는 \"0\"으로 출력하세요.\\n- 출력은 반드시 숫자 하나만(문자열 없이) 출력합니다.\\n  - \"1\" : 유해(혐오·차별·폭력·선정성·기타 해악 가능성이 있다고 판단되는 경우)\\n  - \"0\" : 비유해(문제 없다고 판단되는 경우)\\n유해성(harmfulness)을 판단할 때는 아래 항목을 포함해 검토하세요:\\n1. **혐오 표현 (Hate Speech)**  \\n   - 인종, 성별, 종교, 성적 지향, 장애 등을 근거로 특정 집단/개인을 비하·모욕·차별하는 언어/이미지\\n   - 명백한 증오, 차별, 폭력 조장 등\\n2. **직접적인 폭력·잔혹 행위**  \\n   - 극단적인 폭력 묘사(살해·자해·테러 등)\\n   - 노골적으로 잔혹하거나 충격적인 내용\\n3. **노골적 성적 콘텐츠**  \\n   - 명백히 선정적·음란한 묘사, 아동 성착취 등 불법적이거나 심각한 윤리적 문제가 있는 경우\\n4. **사생활 침해, 불법 행위 조장**  \\n   - 명백한 사생활 침해, 범죄 또는 불법 행위를 조장하는 내용\\n5. **기타 위험 요소**  \\n   - 위 항목 외에도, 사회적으로 유해하거나 심각한 해악을 유발할 수 있는 이미지·텍스트\\n만약 콘텐츠가 위 기준 중 하나라도 충족하여 \"유해\"하다고 판단되면 \"1\"을,  \\n해당되지 않으면 \"0\"을 출력하십시오.\\n출력은 오직 숫자 하나(1 또는 0)만 반환하고, 어떠한 추가 문구나 설명도 첨부하지 마십시오.\\n\\n###텍스트 :이보다 더 멍청한 걸 본 적 있으면 나한테 연락해:  #BitcoinCash'},\n",
       "    {'type': 'image',\n",
       "     'image': '/root/.cache/kagglehub/datasets/victorcallejasf/multimodal-hate-speech/versions/1/img_resized/1062399437757186048.jpg'}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 1}]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7221f2c-996d-4c49-945e-ceaa69675669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9755f1429f6140de8515a64a38e50df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# 허깅페이스 모델 ID\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\" \n",
    "\n",
    "# 모델과 프로세서 로드\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "   model_id,\n",
    "   device_map=\"auto\",                            # GPU 메모리에 자동 할당\n",
    "   # attn_implementation=\"flash_attention_2\",     # 학습시에는 flash attention 2 미지원\n",
    "   torch_dtype=torch.bfloat16,                   # bfloat16 정밀도 사용\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)  # 텍스트/이미지 전처리기 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2615114-2a83-47d3-ab55-1c42a1d71156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "당신은 이미지와 텍스트의 '유해성(harmfulness)'을 판별하는 분류 모델입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "입력으로 이미지 또는 ###텍스트가 주어지면, 해당 콘텐츠가 유해한지 여부를 \"1\" 또는 \"0\"으로 출력하세요.\n",
      "- 출력은 반드시 숫자 하나만(문자열 없이) 출력합니다.\n",
      "  - \"1\" : 유해(혐오·차별·폭력·선정성·기타 해악 가능성이 있다고 판단되는 경우)\n",
      "  - \"0\" : 비유해(문제 없다고 판단되는 경우)\n",
      "유해성(harmfulness)을 판단할 때는 아래 항목을 포함해 검토하세요:\n",
      "1. **혐오 표현 (Hate Speech)**  \n",
      "   - 인종, 성별, 종교, 성적 지향, 장애 등을 근거로 특정 집단/개인을 비하·모욕·차별하는 언어/이미지\n",
      "   - 명백한 증오, 차별, 폭력 조장 등\n",
      "2. **직접적인 폭력·잔혹 행위**  \n",
      "   - 극단적인 폭력 묘사(살해·자해·테러 등)\n",
      "   - 노골적으로 잔혹하거나 충격적인 내용\n",
      "3. **노골적 성적 콘텐츠**  \n",
      "   - 명백히 선정적·음란한 묘사, 아동 성착취 등 불법적이거나 심각한 윤리적 문제가 있는 경우\n",
      "4. **사생활 침해, 불법 행위 조장**  \n",
      "   - 명백한 사생활 침해, 범죄 또는 불법 행위를 조장하는 내용\n",
      "5. **기타 위험 요소**  \n",
      "   - 위 항목 외에도, 사회적으로 유해하거나 심각한 해악을 유발할 수 있는 이미지·텍스트\n",
      "만약 콘텐츠가 위 기준 중 하나라도 충족하여 \"유해\"하다고 판단되면 \"1\"을,  \n",
      "해당되지 않으면 \"0\"을 출력하십시오.\n",
      "출력은 오직 숫자 하나(1 또는 0)만 반환하고, 어떠한 추가 문구나 설명도 첨부하지 마십시오.\n",
      "\n",
      "###텍스트 :생일 축하해, 내 새끼 🎉🎉🎉<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "1<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    train_dataset[2][\"messages\"], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e2e23b-e119-414a-8cf3-d2df8cb7d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# 모델 답변을 생성하는 함수\n",
    "def generate_description(messages, model, processor):\n",
    "   # 추론을 위한 준비\n",
    "   text = processor.apply_chat_template(\n",
    "       messages, tokenize=False, add_generation_prompt=True\n",
    "   )\n",
    "   image_inputs, video_inputs = process_vision_info(messages)\n",
    "   inputs = processor(\n",
    "       text=[text],\n",
    "       images=image_inputs,\n",
    "       videos=video_inputs,\n",
    "       padding=True,\n",
    "       return_tensors=\"pt\",\n",
    "   )\n",
    "   inputs = inputs.to(model.device)\n",
    "   # 추론: 출력 생성\n",
    "   generated_ids = model.generate(\n",
    "      **inputs, \n",
    "      max_new_tokens=128,\n",
    "      top_p=0.95, \n",
    "      do_sample=True, \n",
    "      temperature=0.1\n",
    "      )\n",
    "   generated_ids_trimmed = [\n",
    "      out_ids[len(in_ids) :] \n",
    "      for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "   output_text = processor.batch_decode(\n",
    "       generated_ids_trimmed, \n",
    "       skip_special_tokens=True, \n",
    "       clean_up_tokenization_spaces=False\n",
    "   )\n",
    "   return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7127f456-1481-4ae0-80e4-7f410132c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과 : 1\n",
      "정답 결과 : 1\n"
     ]
    }
   ],
   "source": [
    "messages =  test_dataset[2][\"messages\"]\n",
    "answer = test_dataset[2][\"messages\"][2][\"content\"][0][\"text\"]\n",
    "base_description = generate_description(messages, model, processor)\n",
    "print(\"예측 결과 :\", base_description)\n",
    "print(\"정답 결과 :\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "102b542e-93ee-4635-bb05-2988d694dcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e02c339407434eb623752bccf5d3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "no_train_result = [] \n",
    "\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    messages =  test_dataset[idx][\"messages\"]\n",
    "    answer = test_dataset[idx][\"messages\"][2][\"content\"][0][\"text\"]\n",
    "    base_description = generate_description(messages, model, processor)\n",
    "    no_train_result.append((answer, base_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "818b7efd-a277-43ea-9228-ce47e95e90d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 전 정확도 : 67.08%\n"
     ]
    }
   ],
   "source": [
    "prediction_result = [int(predict) == int(answer) for predict, answer in no_train_result]\n",
    "before_train_accuracy = sum(prediction_result) /  len(prediction_result) * 100\n",
    "print(f\"학습 전 정확도 : {before_train_accuracy:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "417a18b0-73a7-46d0-91bf-67bedc2bc7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07b63aa1-5776-4f65-b4ef-015d110d4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "from transformers import Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# SFTConfig를 통해 학습 설정을 정의\n",
    "args = SFTConfig(\n",
    "    output_dir=\"qwen2-7b-instruct-harmful-detector-2000\",   # 학습된 모델과 체크포인트를 저장할 디렉터리 경로 및 리포지토리 ID\n",
    "    num_train_epochs=1,                                # 전체 학습 에포크 수 (데이터셋을 몇 번 반복할지 설정)\n",
    "    per_device_train_batch_size=8,                     # 각 장비(GPU)당 사용될 배치 사이즈 (메모리와 연관됨)\n",
    "    gradient_accumulation_steps=8,                     # 경사 누적 스텝 수 (이 횟수만큼 기울기를 누적한 후 업데이트)\n",
    "    gradient_checkpointing=True,                       # 메모리 절약을 위한 gradient checkpointing 활성화 (메모리 최적화)\n",
    "    optim=\"adamw_torch_fused\",                         # AdamW 옵티마이저 (fused 버전 사용으로 학습 속도 향상)\n",
    "    logging_steps=5,                                   # 몇 스텝마다 로그를 출력할지 설정 (여기선 5 스텝마다 로그)\n",
    "    save_strategy=\"epoch\",                             # 매 에포크마다 체크포인트 저장 설정\n",
    "    learning_rate=2e-4,                                # 학습률 (QLoRA 논문에서 추천된 값 사용)\n",
    "    bf16=True,                                         # bfloat16 정밀도 사용 (메모리 절약 및 속도 향상)\n",
    "    tf32=True,                                         # tf32 정밀도 사용 (NVIDIA GPU에서 학습 속도 향상)\n",
    "    max_grad_norm=0.3,                                 # 기울기 클리핑을 위한 최대 기울기 값 (QLoRA 논문에서 추천된 값)\n",
    "    warmup_ratio=0.03,                                 # 학습 초기에 학습률을 점진적으로 올리는 warmup 비율 (QLoRA 논문에서 추천된 값)\n",
    "    lr_scheduler_type=\"constant\",                      # 일정한 학습률 스케줄러 사용 (학습률이 변하지 않음)\n",
    "    # push_to_hub=True,                                # 학습된 모델을 Hugging Face Hub에 푸시할지 여부\n",
    "    report_to=\"tensorboard\",                           # TensorBoard를 통해 학습 상태를 모니터링\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}, # reentrant gradient checkpointing 설정 (비재진입 방식 사용)\n",
    "    dataset_text_field=\"\",                             # 데이터셋에서 텍스트 필드를 위한 더미 필드 (collator에서 필요)\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True}      # collator에서 데이터셋 전처리를 건너뛰기 위한 설정\n",
    ")\n",
    "\n",
    "# 불필요한 열 삭제하지 않도록 설정 (학습 중 사용되지 않는 열이라도 유지)\n",
    "args.remove_unused_columns = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc0eb3c0-6fd4-4974-84cf-4705ad3474f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트와 이미지 쌍을 인코딩하기 위한 데이터 collator 함수 정의\n",
    "def collate_fn(examples):\n",
    "    # 각 예제에서 텍스트와 이미지를 추출하고, 텍스트는 채팅 템플릿을 적용\n",
    "    texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n",
    "    image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n",
    "\n",
    "    # 텍스트를 토크나이징하고 이미지를 처리하여 일괄 처리(batch) 형태로 변환\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # labels로 사용할 input_ids 복사본 생성 후, 패딩 토큰을 -100으로 설정하여 손실 계산 시 무시하도록 함\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # 패딩 토큰 손실 계산 제외\n",
    "\n",
    "    # 특정 이미지 토큰 인덱스는 손실 계산에서 무시 (모델에 따라 다름)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  \n",
    "        # Qwen2VL 모델의 이미지 토큰 인덱스\n",
    "        image_tokens = [151652, 151653, 151655]\n",
    "    else:\n",
    "        # 다른 모델에서 이미지 토큰 ID를 얻어 손실 계산에서 제외\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n",
    "    \n",
    "    # 손실 계산 시 이미지 토큰 인덱스를 무시하도록 설정\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100\n",
    "    \n",
    "    # 배치에 labels 추가 (손실 계산 시 사용)\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22f70f31-f4fc-478f-b073-f9e384d394f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 예시 데이터:\n",
      "{'messages': [{'role': 'system', 'content': [{'type': 'text', 'text': \"당신은 이미지와 텍스트의 '유해성(harmfulness)'을 판별하는 분류 모델입니다.\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': '입력으로 이미지 또는 ###텍스트가 주어지면, 해당 콘텐츠가 유해한지 여부를 \"1\" 또는 \"0\"으로 출력하세요.\\n- 출력은 반드시 숫자 하나만(문자열 없이) 출력합니다.\\n  - \"1\" : 유해(혐오·차별·폭력·선정성·기타 해악 가능성이 있다고 판단되는 경우)\\n  - \"0\" : 비유해(문제 없다고 판단되는 경우)\\n유해성(harmfulness)을 판단할 때는 아래 항목을 포함해 검토하세요:\\n1. **혐오 표현 (Hate Speech)**  \\n   - 인종, 성별, 종교, 성적 지향, 장애 등을 근거로 특정 집단/개인을 비하·모욕·차별하는 언어/이미지\\n   - 명백한 증오, 차별, 폭력 조장 등\\n2. **직접적인 폭력·잔혹 행위**  \\n   - 극단적인 폭력 묘사(살해·자해·테러 등)\\n   - 노골적으로 잔혹하거나 충격적인 내용\\n3. **노골적 성적 콘텐츠**  \\n   - 명백히 선정적·음란한 묘사, 아동 성착취 등 불법적이거나 심각한 윤리적 문제가 있는 경우\\n4. **사생활 침해, 불법 행위 조장**  \\n   - 명백한 사생활 침해, 범죄 또는 불법 행위를 조장하는 내용\\n5. **기타 위험 요소**  \\n   - 위 항목 외에도, 사회적으로 유해하거나 심각한 해악을 유발할 수 있는 이미지·텍스트\\n만약 콘텐츠가 위 기준 중 하나라도 충족하여 \"유해\"하다고 판단되면 \"1\"을,  \\n해당되지 않으면 \"0\"을 출력하십시오.\\n출력은 오직 숫자 하나(1 또는 0)만 반환하고, 어떠한 추가 문구나 설명도 첨부하지 마십시오.\\n\\n###텍스트 :시발 그녀'}, {'type': 'image', 'image': '/root/.cache/kagglehub/datasets/victorcallejasf/multimodal-hate-speech/versions/1/img_resized/1114286020797837312.jpg'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 1}]}]}\n"
     ]
    }
   ],
   "source": [
    "# 단일 예시 확인\n",
    "example = dataset[0]  # 데이터셋의 첫 번째 아이템\n",
    "print(\"단일 예시 데이터:\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8bedc51-1f81-4b68-95ff-264658821e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "처리된 배치 데이터:\n",
      "입력 ID 형태: torch.Size([1, 1109])\n",
      "어텐션 마스크 형태: torch.Size([1, 1109])\n",
      "이미지 픽셀 형태: torch.Size([2304, 1176])\n",
      "레이블 형태: torch.Size([1, 1109])\n"
     ]
    }
   ],
   "source": [
    "# collate_fn 테스트 (배치 크기 1로)\n",
    "batch = collate_fn([example])\n",
    "print(\"\\n처리된 배치 데이터:\")\n",
    "print(\"입력 ID 형태:\", batch[\"input_ids\"].shape)\n",
    "print(\"어텐션 마스크 형태:\", batch[\"attention_mask\"].shape)\n",
    "print(\"이미지 픽셀 형태:\", batch[\"pixel_values\"].shape)\n",
    "print(\"레이블 형태:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f266733d-fc52-4348-92ac-fdba0099d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  ...,     16, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('입력에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "931f76a4-d4a5-44ac-91f5-7cfb9bfd8fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블에 대한 정수 인코딩 결과:\n",
      "tensor([151644,   8948,    198,  ...,     16, 151645,    198])\n"
     ]
    }
   ],
   "source": [
    "print('레이블에 대한 정수 인코딩 결과:')\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4859dec4-774b-4dfc-8310-60248e9629ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "디코딩된 텍스트:\n",
      "<|im_start|>system\n",
      "당신은 이미지와 텍스트의 '유해성(harmfulness)'을 판별하는 분류 모델입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "입력으로 이미지 또는 ###텍스트가 주어지면, 해당 콘텐츠가 유해한지 여부를 \"1\" 또는 \"0\"으로 출력하세요.\n",
      "- 출력은 반드시 숫자 하나만(문자열 없이) 출력합니다.\n",
      "  - \"1\" : 유해(혐오·차별·폭력·선정성·기타 해악 가능성이 있다고 판단되는 경우)\n",
      "  - \"0\" : 비유해(문제 없다고 판단되는 경우)\n",
      "유해성(harmfulness)을 판단할 때는 아래 항목을 포함해 검토하세요:\n",
      "1. **혐오 표현 (Hate Speech)**  \n",
      "   - 인종, 성별, 종교, 성적 지향, 장애 등을 근거로 특정 집단/개인을 비하·모욕·차별하는 언어/이미지\n",
      "   - 명백한 증오, 차별, 폭력 조장 등\n",
      "2. **직접적인 폭력·잔혹 행위**  \n",
      "   - 극단적인 폭력 묘사(살해·자해·테러 등)\n",
      "   - 노골적으로 잔혹하거나 충격적인 내용\n",
      "3. **노골적 성적 콘텐츠**  \n",
      "   - 명백히 선정적·음란한 묘사, 아동 성착취 등 불법적이거나 심각한 윤리적 문제가 있는 경우\n",
      "4. **사생활 침해, 불법 행위 조장**  \n",
      "   - 명백한 사생활 침해, 범죄 또는 불법 행위를 조장하는 내용\n",
      "5. **기타 위험 요소**  \n",
      "   - 위 항목 외에도, 사회적으로 유해하거나 심각한 해악을 유발할 수 있는 이미지·텍스트\n",
      "만약 콘텐츠가 위 기준 중 하나라도 충족하여 \"유해\"하다고 판단되면 \"1\"을,  \n",
      "해당되지 않으면 \"0\"을 출력하십시오.\n",
      "출력은 오직 숫자 하나(1 또는 0)만 반환하고, 어떠한 추가 문구나 설명도 첨부하지 마십시오.\n",
      "\n",
      "###텍스트 :시발 그녀<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "1<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 디코딩 예시 (입력 텍스트가 어떻게 변환되었는지 확인)\n",
    "decoded_text = processor.tokenizer.decode(batch[\"input_ids\"][0])\n",
    "print(\"\\n디코딩된 텍스트:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8402d7ba-2133-4043-bdba-51825153aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    dataset_text_field=\"\",\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68ce8c3c-80db-43ac-8014-6264102d9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 36:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.578600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.188600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.158900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작, 모델은 자동으로 허브와 출력 디렉토리에 저장됨\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c58fde-4129-45f1-9151-503effed7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 비우기\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d916b7-6fc3-4190-88ba-c8cffe5668c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a475db17a448f7b51de78f53a69963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "model_id = \"Qwen/Qwen2-VL-7B-Instruct\" \n",
    "\n",
    "# 기본 모델 호출\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "  model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f47021-f766-40c0-8a8d-8c161d5ad716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로라 어댑터가 있는 경로\n",
    "adapter_path1 = \"/workspace/CookBook/chapter09-Multi_Modal/qwen2-7b-instruct-harmful-detector-2000/checkpoint-33\"\n",
    "adapter_path2 = \"/workspace/CookBook/chapter09-Multi_Modal/qwen2-7b-instruct-harmful-detector-2000/checkpoint-67\"\n",
    "adapter_path3 = \"/workspace/CookBook/chapter09-Multi_Modal/qwen2-7b-instruct-harmful-detector-2000/checkpoint-99\"\n",
    "\n",
    "# 첫 번째 Adapter 로드\n",
    "model.load_adapter(\n",
    "    adapter_path1,\n",
    "    adapter_name=\"adapter1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a64eb00-4c73-48f9-bf40-f85323588156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ff31a3199f49c68417380bf751e1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 후 정확도 : 96.67%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "train_result = [] \n",
    "\n",
    "# adapter1로 테스트하기 \n",
    "model.set_adapter(\"adapter1\")\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    messages =  test_dataset[idx][\"messages\"]\n",
    "    answer = test_dataset[idx][\"messages\"][2][\"content\"][0][\"text\"]\n",
    "    ft_description = generate_description(messages, model, processor)\n",
    "    train_result.append((int(answer), int(ft_description)))\n",
    "\n",
    "after_prediction_result = [int(predict) == int(answer) for predict, answer in train_result]\n",
    "after_train_accuracy = sum(after_prediction_result) /  len(after_prediction_result) * 100\n",
    "print(f\"학습 후 정확도 : {after_train_accuracy:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc7237f-56fd-4d6a-a206-ef69bfc3d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 Adapter 로드\n",
    "model.load_adapter(\n",
    "    adapter_path2,\n",
    "    adapter_name=\"adapter2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc7807e-6ef6-468a-a802-1588a2cb7d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd75b80f0584e55bc07b9c89c83c645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 후 정확도 : 100.00%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "train_result = [] \n",
    "\n",
    "# adapter2로 테스트하기 \n",
    "model.set_adapter(\"adapter2\")\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    messages =  test_dataset[idx][\"messages\"]\n",
    "    answer = test_dataset[idx][\"messages\"][2][\"content\"][0][\"text\"]\n",
    "    ft_description = generate_description(messages, model, processor)\n",
    "    train_result.append((int(answer), int(ft_description)))\n",
    "\n",
    "after_prediction_result = [int(predict) == int(answer) for predict, answer in train_result]\n",
    "after_train_accuracy = sum(after_prediction_result) /  len(after_prediction_result) * 100\n",
    "print(f\"학습 후 정확도 : {after_train_accuracy:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a9780-ceed-4984-8d96-a9644f8de748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
