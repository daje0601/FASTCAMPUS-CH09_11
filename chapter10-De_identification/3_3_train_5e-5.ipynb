{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import re\n",
    "\n",
    "import json \n",
    "import torch\n",
    "import json_repair\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "from pqdm.processes import pqdm\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForCausalLM, \n",
    "                          TrainingArguments, \n",
    "                          pipeline)\n",
    "\n",
    "\n",
    "# 제작한 데이터세 불러오기 \n",
    "file_list = glob(\"./data/*.csv\")\n",
    "print(file_list)\n",
    "\n",
    "df = pd.concat([pd.read_csv(file) for file in file_list])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_placeholder_mapping(original_text, transformed_text, allowed_types):\n",
    "    allowed_pattern = re.compile(r'\\[(' + '|'.join(allowed_types) + r')\\d*\\]')\n",
    "    generic_pattern = re.compile(r'(\\[[^]]+\\])')\n",
    "\n",
    "    mapping = {}\n",
    "\n",
    "    orig_lines = original_text.splitlines()\n",
    "    trans_lines = transformed_text.splitlines()\n",
    "    n_lines = min(len(orig_lines), len(trans_lines))\n",
    "\n",
    "    for idx in range(n_lines):\n",
    "        orig_line = orig_lines[idx]\n",
    "        trans_line = trans_lines[idx]\n",
    "\n",
    "        parts = re.split(generic_pattern, trans_line)\n",
    "        orig_pos = 0\n",
    "\n",
    "        for i, part in enumerate(parts):\n",
    "            if allowed_pattern.match(part):\n",
    "                # placeholder 발견\n",
    "                # 다음 literal을 찾음\n",
    "                next_literal = parts[i + 1] if i + 1 < len(parts) else ''\n",
    "                \n",
    "                # 다음 literal이 존재하면, 그 literal까지의 텍스트를 추출\n",
    "                if next_literal:\n",
    "                    next_idx = orig_line.find(next_literal, orig_pos)\n",
    "                    if next_idx != -1:\n",
    "                        replaced_text = orig_line[orig_pos:next_idx]\n",
    "                        orig_pos = next_idx\n",
    "                    else:\n",
    "                        # 다음 literal을 못 찾으면 끝까지\n",
    "                        replaced_text = orig_line[orig_pos:]\n",
    "                        orig_pos = len(orig_line)\n",
    "                else:\n",
    "                    # 다음 literal이 없으면 남은 텍스트 전체\n",
    "                    replaced_text = orig_line[orig_pos:]\n",
    "                    orig_pos = len(orig_line)\n",
    "\n",
    "                replaced_text = replaced_text.strip()\n",
    "                if replaced_text:\n",
    "                    mapping[replaced_text] = part\n",
    "\n",
    "            else:\n",
    "                # literal인 경우, 원본에서 위치 업데이트\n",
    "                found_idx = orig_line.find(part, orig_pos)\n",
    "                if found_idx != -1:\n",
    "                    orig_pos = found_idx + len(part)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"origin_data\"].iloc[20])\n",
    "print(\"--------------\")\n",
    "print(df[\"anonymized_data\"].iloc[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"origin_data\"].iloc[-2])\n",
    "print(\"--------------\")\n",
    "print(df[\"anonymized_data\"].iloc[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mapping\"] = df[\"mapping\"].map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "def get_chat_format(element):\n",
    "    system_prompt = \"너는 개인정보를 비식별화하는 Assistant야. 너는 주어진 데이터를 바탕으로 개인정보를 비식별화하는 작업을 해야해.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": element[\"origin_data\"]},\n",
    "            {\"role\": \"assistant\", \"content\": element[\"anonymized_data\"]},\n",
    "        ], \n",
    "        \"label\": element[\"mapping\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(get_chat_format, remove_columns=dataset.features, batched=False)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_alpha = 128\n",
    "lora_r = 256\n",
    "learning_rate = 5e-5\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=0.05,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"up_proj\",\n",
    "            \"o_proj\",\n",
    "            \"k_proj\",\n",
    "            \"down_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "save_dir = f\"./model/model_{learning_rate}_alpha-{lora_alpha}_r-{lora_r}\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{save_dir}\", \n",
    "    num_train_epochs=5,          \n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,  \n",
    "    optim=\"adamw_torch_fused\",    \n",
    "    logging_steps=2,            \n",
    "    save_strategy=\"epoch\",        \n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,                    \n",
    "    tf32=True,                    \n",
    "    max_grad_norm=0.3,            \n",
    "    warmup_ratio=0.03,            \n",
    "    lr_scheduler_type=\"constant\", \n",
    "    push_to_hub=False,             \n",
    "    report_to=\"wandb\",            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right'  \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"length\"] = df[\"origin_data\"].apply(len) + df[\"anonymized_data\"].apply(len)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df[\"length\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Text Length\")\n",
    "plt.xlabel(\"Text Length (characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    max_seq_length=2400,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(f\"{save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/workspace/FASTCAMPUS-CH09_11/chapter10-De_identification/model/model_5e-05_alpha-128_r-256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델을 경로를 지정합니다.\n",
    "peft_model_id = f\"{save_dir}\"\n",
    "\n",
    "# PEFT 어댑터를 통해 사전 학습된 모델을 로드합니다.\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 토크나이저 로드합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "tokenizer.padding_side = 'right'  \n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(dataset[\"test\"][-2][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=512, \n",
    "    do_sample=True, \n",
    "    temperature=0.1, \n",
    "    top_k=40, \n",
    "    top_p=0.9, \n",
    "    eos_token_id=pipe.tokenizer.eos_token_id, \n",
    "    pad_token_id=pipe.tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = outputs[0][\"generated_text\"][len(prompt):]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = dataset[\"test\"][-2][\"messages\"][1][\"content\"]\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_result = extract_placeholder_mapping(\n",
    "    input_text, \n",
    "    output_text, \n",
    "    allowed_types=(\n",
    "        \"PERSON\", \"CONTACT\", \"ADDRESS\", \"ACCOUNT\", \"DATEOFBIRTH\", \n",
    "        \"EMAIL\", \"LOCATION\", \"KAKO_ID\", \"TIWTTER_ID\", \"TELEGRAM_ID\"))\n",
    "\n",
    "print(mapping_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
