{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448e8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb6284-ea43-4b6b-ab7d-cbd9844890f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/text_to_sql/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [06:58<00:00, 104.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# 제로샷 테스트를 하기 위해 모델을 다운받고, 인퍼런스를 실행합니다. \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"allganize/Llama-3-Alpha-Ko-8B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": 'Task:라틴 아메리카/카리브해 지역의 인구가 783(7.5%)가 될 때, 아시아의 인구는 얼마가 될까요?\\nSQL Table:CREATE TABLE table_22767 (\\n    \"Year\" real,\\n    \"World\" real,\\n    \"Asia\" text,\\n    \"Africa\" text,\\n    \"Europe\" text,\\n    \"Latin America/Caribbean\" text,\\n    \"Northern America\" text,\\n    \"Oceania\" text\\n)\\nQuery:'}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb60a96-a44a-4a0d-92b9-5b3103e4806e",
   "metadata": {},
   "source": [
    "# 제로샷 테스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde895e9-8175-44c2-adaa-2f4a67cd8d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the population of Asia when the population of Latin America/Caribbean is 783 (7.5% of the total world population), we need to first find the total world population and then calculate the population of Asia.\n",
      "\n",
      "Assuming the data is stored in a SQL table `table_22767`, we can use the following query to find the total world population and the population of Asia:\n",
      "\n",
      "```sql\n",
      "SELECT\n",
      "    \"Year\",\n",
      "    \"World\",\n",
      "    \"Asia\",\n",
      "    \"Africa\",\n",
      "    \"Europe\",\n",
      "    \"Latin America/Caribbean\",\n",
      "    \"Northern America\",\n",
      "    \"Oceania\"\n",
      "FROM\n",
      "    table_22767\n",
      "WHERE\n",
      "    \"Latin America/Caribbean\" = 783;\n",
      "```\n",
      "\n",
      "If the data is not available in the table, we need to calculate the total world population and the population of Asia based on the given data. Let's assume the total world population is `P` and the population of Latin America/Caribbean is `L`.\n",
      "\n",
      "The total world population is the sum of the population of all regions:\n",
      "\n",
      "`P = Asia + Africa + Europe + Latin America/Caribbean + Northern America + Oceania`\n",
      "\n",
      "The population of Latin America/Caribbean is 7.5% of the total world population:\n",
      "\n",
      "`L = 0.075 * P`\n",
      "\n",
      "We can now substitute the value of `L` in the first equation to find the total world population `P`:\n",
      "\n",
      "`P = Asia + Africa + Europe + 783 + Northern America + Oceania`\n",
      "\n",
      "We can further simplify the equation by substituting `L` for `0.075 * P`:\n",
      "\n",
      "`P = Asia + Africa + Europe + 783 + Northern America + Oceania`\n",
      "\n",
      "`P = Asia + Africa + Europe + 0.075 * P + Northern America + Oceania`\n",
      "\n",
      "`P - 0.075 * P = Asia + Africa + Europe + Northern America + Oceania`\n",
      "\n",
      "`0.925 * P = Asia + Africa + Europe + Northern America + Oceania`\n",
      "\n",
      "To find the population of Asia, we need to divide both sides by 0.925:\n",
      "\n",
      "`Asia + Africa + Europe + Northern America + Oceania = 0.925 * P`\n",
      "\n",
      "`Asia + Africa + Europe + Northern America + Oceania = 0.925 * (Asia + Africa + Europe + Northern America + Oceania) / 0.925`\n",
      "\n",
      "`Asia = 0.925 * (Asia + Africa + Europe + Northern America\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaaf501-2e89-4909-a210-8f454b06028c",
   "metadata": {},
   "source": [
    "### 정답 \n",
    "SELECT \"Asia\" FROM table_22767 WHERE \"Latin America/Caribbean\" = \\'783 (7.5%)\\'\n",
    "\n",
    "### 생성 결과\n",
    "SELECT (783 / 0.075) AS World_Population, (0.15 * (783 / 0.075)) AS Asia_Population;\n",
    "\n",
    "잘못된 결과를 생성하고 있고 우리가 원하지 않는 문장들이 포함되어 있습니다.   \n",
    "Fine-Tuning을 통해 이러한 문제를 해결해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc644522-d263-4768-a139-a4d8fd3cd0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import trl\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "\n",
    "import wandb\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments,\n",
    "                          pipeline)\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5079e699-d44d-44a5-b839-fb5e542737a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version       : 2.5.1+cu124\n",
      "Transformers version  : 4.47.1\n",
      "TRL version           : 0.13.0\n",
      "CUDA available        : True\n",
      "CUDA version      : 12.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version       : {torch.__version__}\")\n",
    "print(f\"Transformers version  : {transformers.__version__}\")\n",
    "print(f\"TRL version           : {trl.__version__}\")\n",
    "print(f\"CUDA available        : {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version      : {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcf35f-c1cc-4c37-b946-28759da62a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\n",
    "  token=\"Huggingface_Token\", # 여기에 토큰 추가 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2dae205-17f5-4e2f-9888-55e61dfe8ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'response', 'source', 'text', 'ko_instruction'],\n",
       "        num_rows: 262208\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = datasets.load_dataset(\"Clinton/text-to-sql-v1\")\n",
    "dataset = datasets.load_dataset(\"daje/kotext-to-sql-v1\")\n",
    "dataset     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13429c8-5d5d-421c-9930-15f496d90c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910 13\n"
     ]
    }
   ],
   "source": [
    "def add_length_column(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    df[\"total_length\"] = 0\n",
    "    for column_name in [\"ko_instruction\", \"input\", \"response\"]:\n",
    "        num_words = df[column_name].astype(str).str.split().apply(len)\n",
    "        df[\"total_length\"] += num_words\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_length_column(dataset[\"train\"])\n",
    "\n",
    "def filter_by_total_length(df, difficulty, number_of_samples, random_state=8888):  # random_state 추가\n",
    "    if difficulty == \"easy\":\n",
    "        return df[df[\"total_length\"].between(10, 100)].sample(n=number_of_samples, random_state=random_state)  # iloc 대신 sample 사용\n",
    "    elif difficulty == \"moderate\":\n",
    "        return df[df[\"total_length\"].between(101, 300)].sample(n=number_of_samples, random_state=random_state)\n",
    "    elif difficulty == \"difficult\":\n",
    "        return df[df[\"total_length\"].between(301, 1000)].sample(n=number_of_samples, random_state=random_state)\n",
    "\n",
    "print(max(df[\"total_length\"].to_list()), min(df[\"total_length\"].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1563b736-67bb-416c-8cd4-07541262cf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 7), (5000, 7), (2000, 7), (12000, 8))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy = filter_by_total_length(df, \"easy\", 10000)\n",
    "medium = filter_by_total_length(df, \"moderate\", 10000)\n",
    "hard = filter_by_total_length(df, \"difficult\", 2000)\n",
    "dataset = pd.concat([easy, medium, hard])\n",
    "dataset = dataset.sample(frac=1, random_state=8888)  # random_state 추가\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "easy.shape, medium.shape, hard.shape, dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c05b35-b425-442f-ae7a-cf12d4ed3be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7529b59a735473bb1cce8a7e4ff2306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trl docs에 보면 이와 같은 방식으로 SFT Trainer용 데이터를 만들 수 있습니다.\n",
    "# docs에서는 eos_token을 별도로 추가하라는 안내는 없지만, 저자는 습관적으로 eos_token을 붙혀줍니다.\n",
    "def get_chat_format(element):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful programmer assistant that excels at SQL. \"\n",
    "        \"Below are sql tables schemas paired with instruction that describes a task. \"\n",
    "        \"Using valid SQLite, write a response that appropriately completes the request for the provided tables.\"\n",
    "    )\n",
    "    user_prompt = \"### instruction:{ko_instruction} ### Input:{input} ### response:\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format_map(element)},\n",
    "            {\"role\": \"assistant\", \"content\": element[\"response\"]+tokenizer.eos_token},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "tokenizer.padding_side = 'right'                      \n",
    "\n",
    "def tokenize(element):\n",
    "    # 참고 코드에서처럼, apply_chat_template 사용\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        get_chat_format(element)[\"messages\"],  # get_chat_format 반환 형식을 맞춰줍니다.\n",
    "        tokenize=False\n",
    "    )\n",
    "    outputs = tokenizer(formatted)\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "    \n",
    "# 데이터를 일괄적으로 대화형식으로 변경합니다.\n",
    "temp_dataset = dataset.map(get_chat_format, remove_columns=dataset.features, batched=False)\n",
    "\n",
    "# train과 test 데이터를 0.9와 0.1로 분할합니다.\n",
    "temp_dataset = temp_dataset.train_test_split(test_size=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119db772-aa1a-48b0-87b6-9693e485a003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 11400\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 600\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정성적으로 변환되었는지 확인합니다.\n",
    "temp_dataset[\"train\"], temp_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4c84ad-924e-4586-be64-320dbe12a7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful programmer assistant that excels at SQL. Below are sql tables schemas paired with instruction that describes a task. Using valid SQLite, write a response that appropriately completes the request for the provided tables.',\n",
       "   'role': 'system'},\n",
       "  {'content': '### instruction:\\'어떤 시즌이 유일하게 1라운드였나요?\\' ### Input:CREATE TABLE table_204_985 (\\n    id number,\\n    \"season\" text,\\n    \"competition\" text,\\n    \"round\" text,\\n    \"club\" text,\\n    \"home\" text,\\n    \"away\" text,\\n    \"aggregate\" text\\n) ### response:',\n",
       "   'role': 'user'},\n",
       "  {'content': 'SELECT \"season\" FROM table_204_985 WHERE \"round\" = \\'1\\'<|end_of_text|>',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42d32be-806a-4eb0-acaa-5cebe9959bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful programmer assistant that excels at SQL. Below are sql tables schemas paired with instruction that describes a task. Using valid SQLite, write a response that appropriately completes the request for the provided tables.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### instruction:'어떤 시즌이 유일하게 1라운드였나요?' ### Input:CREATE TABLE table_204_985 (\n",
      "    id number,\n",
      "    \"season\" text,\n",
      "    \"competition\" text,\n",
      "    \"round\" text,\n",
      "    \"club\" text,\n",
      "    \"home\" text,\n",
      "    \"away\" text,\n",
      "    \"aggregate\" text\n",
      ") ### response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "SELECT \"season\" FROM table_204_985 WHERE \"round\" = '1'<|end_of_text|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "temp = tokenizer.apply_chat_template(temp_dataset[\"train\"][0][\"messages\"], tokenizer=False)\n",
    "print(tokenizer.decode(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "469f29ce-6dcc-4671-aedb-00ad282f890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b305b4e-fed0-4772-98af-19c98e403142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc853e9f9834a04ac0ceeebea89fb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa6d4c03a7c43068b5338153c9f0638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21a11ea5aa24d4eb90c8f939b065e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48ad3453b18473c839354e074e1ffa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880a24aaec85421e9fceb5b7b12ea2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trl docs에 보면 이와 같은 방식으로 SFT Trainer용 데이터를 만들 수 있습니다.\n",
    "# docs에서는 eos_token을 별도로 추가하라는 안내는 없지만, 저자는 습관적으로 eos_token을 붙혀줍니다.\n",
    "def get_chat_format(element):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful programmer assistant that excels at SQL. \"\n",
    "        \"Below are sql tables schemas paired with instruction that describes a task. \"\n",
    "        \"Using valid SQLite, write a response that appropriately completes the request for the provided tables.\"\n",
    "    )\n",
    "    user_prompt = \"### instruction:{ko_instruction} ### Input:{input} ### response:\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt.format_map(element)},\n",
    "            {\"role\": \"assistant\", \"content\": element[\"response\"]+tokenizer.eos_token},  # 여기 닫는 괄호 추가\n",
    "        ]\n",
    "    }\n",
    "\n",
    "tokenizer.padding_side = 'right'                      \n",
    "\n",
    "def apply_chat_format(element):\n",
    "    \"\"\"\n",
    "    1) get_chat_format(element)를 호출해서\n",
    "       messages를 생성한 뒤, Dataset에 저장할 dict로 반환합니다.\n",
    "    \"\"\"\n",
    "    chat_format = get_chat_format(element)  # get_chat_format은 원본 코드 그대로 사용\n",
    "    return {\n",
    "        \"messages\": chat_format[\"messages\"]\n",
    "    }\n",
    "\n",
    "def tokenize_messages(element):\n",
    "    \"\"\"\n",
    "    2) apply_chat_template + tokenizer(...)를 통해\n",
    "       input_ids와 attention_mask를 만들어 반환합니다.\n",
    "    \"\"\"\n",
    "    # 위 단계에서 \"messages\"가 이미 Dataset에 들어가있다고 가정\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        element[\"messages\"],  # messages 리스트\n",
    "        tokenize=False\n",
    "    )\n",
    "    outputs = tokenizer(formatted)\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# train과 test 데이터를 0.9와 0.1로 분할합니다.\n",
    "dataset = dataset.map(\n",
    "    apply_chat_format,\n",
    "    batched=False,\n",
    "    remove_columns=dataset.features,  # 원하시면 제거\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    tokenize_messages,\n",
    "    batched=False,\n",
    "    remove_columns=[\"messages\"],  # 이제 messages를 더이상 쓰지 않는다면 제거\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db814ead-f19d-4b76-8a28-a368c7a9d6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 11400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced1bad9-54b6-4cc1-ade5-43708589a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "# 만약 실제로 줄바꿈까지 포함되어 있다면 \\n까지 넣어줘야 합니다.\n",
    "# response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template_ids,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d937987e-27c6-46a9-8561-31e164ab132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,                            \n",
    "        lora_dropout=0.05,                         # Lora 학습 때 사용할 dropout 확률을 지정합니다. 드롭아웃 확률은 과적합 방지를 위해 학습 중 무작위로 일부 뉴런을 비활성화하는 비율을 지정합니다.\n",
    "        r=256,                                     # Lora의 저차원 공간의 랭크를 지정합니다. 랭크가 높을수록 모델의 표현력이 증가하지만, 계산 비용도 증가합니다.\n",
    "        bias=\"none\",                               # Lora 적용 시 바이어스를 사용할지 여부를 설정합니다. \"none\"으로 설정하면 바이어스를 사용하지 않습니다.\n",
    "        target_modules=[\"q_proj\", \"o_proj\",        # Lora를 적용할 모델의 모듈 리스트입니다.\n",
    "                        \"k_proj\", \"v_proj\"\n",
    "                        \"up_proj\", \"down_proj\",\n",
    "                        \"gate_proj\",\n",
    "                        ],\n",
    "        task_type=\"CAUSAL_LM\",                     # 미세 조정 작업 유형을 CAUSAL_LM으로 지정하여 언어 모델링 작업을 수행합니다.\n",
    ")\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama3-12000_ko\", # 모델 저장 및 허브 업로드를 위한 디렉토리 지정 합니다.\n",
    "    num_train_epochs=1,                   # number of training epochs\n",
    "    # max_steps=100,                          # 100스텝 동안 훈련 수행합니다.\n",
    "    per_device_train_batch_size=1,          # 배치 사이즈 설정 합니다.\n",
    "    gradient_accumulation_steps=2,          # 4스텝마다 역전파 및 가중치 업데이트합니다.\n",
    "    gradient_checkpointing=False,            # 메모리 절약을 위해 그래디언트 체크포인팅 사용합니다.\n",
    "    optim=\"adamw_torch_fused\",              # 메모리 효율화할 수 있는 fused AdamW 옵티마이저 사용합니다.\n",
    "    logging_steps=1000,                       # 10스텝마다 로그 기록합니다.\n",
    "    save_strategy=\"steps\",                  # 매 에폭마다 체크포인트 저장합니다.\n",
    "    learning_rate=5e-5,                     # 학습률 2e-4로 설정 (QLoRA 논문 기반)합니다.\n",
    "    bf16=True,                              # 정밀도 설정으로 학습 속도 향상합니다.\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,                      # 그래디언트 클리핑 값 0.3으로 설정합니다.\n",
    "    warmup_ratio=0.03,                      # 워밍업 비율 0.03으로 설정 (QLoRA 논문 기반)합니다.\n",
    "    lr_scheduler_type=\"constant\",           # 일정한 학습률 스케줄러 사용합니다.\n",
    "    push_to_hub=False,                       # 훈련된 모델을 Hugging Face Hub에 업로드합니다.\n",
    "    report_to=\"wandb\",                      # wandb로 매트릭 관찰합니다.\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219029f0-b5fc-4c91-ac39-e4cbec97c955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaje0601\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250109_075058-qkbpawil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daje0601/huggingface/runs/qkbpawil' target=\"_blank\">llama3-12000_ko</a></strong> to <a href='https://wandb.ai/daje0601/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daje0601/huggingface' target=\"_blank\">https://wandb.ai/daje0601/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daje0601/huggingface/runs/qkbpawil' target=\"_blank\">https://wandb.ai/daje0601/huggingface/runs/qkbpawil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5700' max='5700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5700/5700 25:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5700, training_loss=0.2011904880456757, metrics={'train_runtime': 1518.2468, 'train_samples_per_second': 7.509, 'train_steps_per_second': 3.754, 'total_flos': 2.4621672351178752e+17, 'train_loss': 0.2011904880456757, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer를 학습합니다.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae404e1e-e459-450d-b2ea-42ae54e6516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 초기화\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b01b8d-ba39-47ec-a8bc-b56e19911f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707bb01f-83a1-44b2-835b-4ada72d42fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484635d19e1b4e4080404dfda1b8c69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99aa7e0-4e73-4b48-9e97-d47a69a88072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are a helpful programmer assistant that excels at SQL. Below are sql tables schemas paired with instruction that describes a task. Using valid SQLite, write a response that appropriately completes the request for the provided tables.',\n",
       "  'role': 'system'},\n",
       " {'content': \"### instruction:'5 7, 2 6'의 점수를 가진 대회는 무엇인가요? ### Input:CREATE TABLE table_name_6 (\\n    tournament VARCHAR,\\n    score VARCHAR\\n) ### response:\",\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0][\"messages\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc02e0e-8ed3-4a19-a896-642b83115032",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "sql_chat_completion = client.chat.completions.create(\n",
    "    model=\"lora_adapter1\",\n",
    "    messages=eval_dataset[idx][\"messages\"][:2],\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ba06bf-6b45-4760-a851-85186ba56dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'SELECT tournament FROM table_name_6 WHERE score = \"5–7, 2–6\"<|end_of_text|>',\n",
       " 'role': 'assistant'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[idx][\"messages\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a821e81-e17a-46c0-9e92-827bf46368fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT tournament FROM table_name_6 WHERE score = \"5–7, 2–6\"\n"
     ]
    }
   ],
   "source": [
    "print(sql_chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e867db3-c312-445d-b43f-cfed21b4e0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ae92963784a6fb6ca390a52cc2139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "result = [] \n",
    "for idx in tqdm(range(len(eval_dataset))):\n",
    "    sql_chat_completion = client.chat.completions.create(\n",
    "    model=\"lora_adapter1\",\n",
    "    messages=eval_dataset[idx][\"messages\"][:2],\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    "    stop=[\"<|eot_id|>\"]\n",
    "    )\n",
    "    result.append((eval_dataset[idx][\"messages\"][2][\"content\"], sql_chat_completion.choices[0].message.content))\n",
    "\n",
    "print(\"완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d66126-1e7f-4e6d-bc99-d329d93b1886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.50%\n"
     ]
    }
   ],
   "source": [
    "generated_result = [temp[0].replace(\"<|end_of_text|>\", \"\").strip() == temp[1].replace(\"<|end_of_text|>\", \"\").strip() for temp in result]\n",
    "\n",
    "# Exact Match 기준으로 ACC(정확도)를 구합니다.\n",
    "accuracy = sum(generated_result)/len(generated_result)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a130b11-1934-465c-bbfa-c72ec3a0b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0c9980-7f2f-4164-9af3-0f5d6c7cb1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e641e43fda34538a6044b88d8090522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = [] \n",
    "for idx in tqdm(range(len(eval_dataset))):\n",
    "    # 1) system / user 메시지 content를 합침\n",
    "    #    eval_dataset[idx][\"messages\"][:2] -> [system, user] 두 개의 메시지\n",
    "    combined_content = \"\\n\".join(msg[\"content\"] for msg in eval_dataset[idx][\"messages\"][:2])\n",
    "\n",
    "    temp.append(combined_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c841a52d-0238-4769-b9e6-63fe4deb13d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"You are a helpful programmer assistant that excels at SQL. Below are sql tables schemas paired with instruction that describes a task. Using valid SQLite, write a response that appropriately completes the request for the provided tables.\\n### instruction:'5 7, 2 6'의 점수를 가진 대회는 무엇인가요? ### Input:CREATE TABLE table_name_6 (\\n    tournament VARCHAR,\\n    score VARCHAR\\n) ### response:\", 'SELECT tournament FROM table_name_6 WHERE score = \"5–7, 2–6\"<|end_of_text|>', 'SELECT tournament FROM table_name_6 WHERE score = \"5–7, 2–6\"')\n"
     ]
    }
   ],
   "source": [
    "combined_list = []\n",
    "\n",
    "for prompt_str, (gold_answer, generated_answer) in zip(temp, result):\n",
    "    combined_list.append((prompt_str, gold_answer, generated_answer))\n",
    "\n",
    "# 이제 combined_list[i] = (프롬프트, 정답, 생성결과)\n",
    "print(combined_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44d0e7f-2b28-405d-b0e2-9d78e18518cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_evaluation = combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77077933-a917-4a0a-9d6d-445a54da32c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\": 0, \"explanation\": \"두 쿼리는 의미적으로 동일한 결과를 반환하지 않습니다. 생성된 쿼리는 \\\"robert tower\\\"라는 건물이 있는 주/지역을 찾고 있으며, 정답 쿼리는 \\\"lovett tower\\\"라는 건물이 있는 주/지역을 찾고 있습니다. 문제에서 요구한 건물(\\\"lovett tower\\\")명의 철자가 두 쿼리에서 다릅니다. 따라서 두 쿼리는 반환하는 결과가 다를 수 있습니다.\"}\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "# gpt-4o-mini를 사용해서 문제와 정답과 생성된 결과를 넣고, 같은 쿼리인지 확인\n",
    "# OpenAI API 키 설정 (환경 변수에서 가져오거나 직접 입력)\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def one_compare_sql_semantics(problem_description, generated_query, ground_truth_query):\n",
    "    # ChatGPT에게 물어볼 프롬프트 작성\n",
    "    prompt = f\"\"\"다음 문제와 두 SQL 쿼리가 의미적으로 동일한 결과를 반환하는지 판단해주세요:\n",
    "\n",
    "    문제 설명: {problem_description}\n",
    "\n",
    "    생성된 쿼리:\n",
    "    {generated_query}\n",
    "\n",
    "    정답 쿼리:\n",
    "    {ground_truth_query}\n",
    "\n",
    "    두 쿼리가 문제에 대해 의미적으로 동일한 결과를 반환한다면 1로 대답하고,\n",
    "    그렇지 않다면 0라고 대답한 후 차이점을 설명해주세요.\n",
    "    쿼리의 구조나 사용된 함수가 다르더라도 결과가 같다면 의미적으로 동일하다고 판단해주세요.\"\"\"\n",
    "\n",
    "    # ChatGPT API 호출\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # 또는 사용 가능한 최신 모델\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that compares the semantic meaning of SQL queries in the context of a given problem.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ChatGPT의 응답 추출\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 결과 처리\n",
    "    is_correct = 1 if answer.lower().startswith(\"yes\") else 0\n",
    "    explanation = answer[3:] if is_correct == 1 else answer[2:]\n",
    "\n",
    "    # JSON 형식으로 결과 반환\n",
    "    result = {\n",
    "        \"answer\": is_correct,\n",
    "        \"explanation\": explanation.strip()\n",
    "    }\n",
    "\n",
    "    return json.dumps(result, ensure_ascii=False)\n",
    "\n",
    "# 사용 예시\n",
    "\n",
    "problem = openai_evaluation[1][0]\n",
    "truth = openai_evaluation[1][1]\n",
    "generated = openai_evaluation[1][2]\n",
    "\n",
    "result = one_compare_sql_semantics(problem, generated, truth)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a228381-aa80-408c-a4ba-939779261651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/text_to_sql/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "QUEUEING TASKS | : 100%|██████████| 10/10 [00:00<00:00, 317.39it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 10/10 [00:00<00:00, 1129.11it/s]\n",
      "COLLECTING RESULTS | : 100%|██████████| 10/10 [00:00<00:00, 52626.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from pqdm.processes import pqdm\n",
    "\n",
    "# 폴더 생성\n",
    "os.makedirs(\"/workspace/openai_result_llama_600_ko\", exist_ok=True)\n",
    "\n",
    "# OpenAI 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "client = OpenAI()\n",
    "\n",
    "def compare_sql_semantics(idx):\n",
    "    save_path = f\"/workspace/openai_result_llama_600_ko/result_{idx}.json\"\n",
    "    \n",
    "    # openai_evaluation 사용 (generated_result 대신)\n",
    "    item = openai_evaluation[idx]\n",
    "    problem_description, generated_query, ground_truth_query = item\n",
    "    \n",
    "    prompt = f\"\"\"다음 문제와 두 SQL 쿼리가 의미적으로 동일한 결과를 반환하는지 판단해주세요:\n",
    "    문제 설명: {problem_description}\n",
    "    생성된 쿼리:\n",
    "    {generated_query}\n",
    "    정답 쿼리:\n",
    "    {ground_truth_query}\n",
    "    두 쿼리가 문제에 대해 의미적으로 동일한 결과를 반환한다면 answer에 \"1\"라고 대답하고,\n",
    "    그렇지 않다면 \"0\"라고 대답한 후 차이점을 explanation에 적으세요.\n",
    "    쿼리의 구조나 사용된 함수가 다르더라도 결과가 같다면 의미적으로 동일하다고 판단해주세요.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant that compares the semantic meaning of SQL queries in the context of a given problem.\n",
    "                return json format below:\n",
    "                {\n",
    "                    \"answer\": \"...\",\n",
    "                    \"explanation\": \"...\"\n",
    "                }\n",
    "                \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # 파일 저장 시도 시 에러 출력\n",
    "        try:\n",
    "            with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(answer, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Successfully saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file {save_path}: {str(e)}\")\n",
    "            \n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing index {idx}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 실행\n",
    "indexed_openai_evaluation = list(range(len(openai_evaluation)))\n",
    "results = pqdm(indexed_openai_evaluation, compare_sql_semantics, n_jobs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1473933d-08bf-4d60-93b1-c9bbc28d5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.17%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_result = []\n",
    "for result in results:\n",
    "    json_result.append(json.loads(result))\n",
    "\n",
    "df = pd.DataFrame(json_result)\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].map(lambda x : int(x))\n",
    "\n",
    "after_accuracy = df[\"answer\"].sum() / len(df[\"answer\"])\n",
    "print(f\"Accuracy: {after_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcadfe4-6ef4-4523-b04c-5ffe50294e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_sql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
